{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "taB4qv1gKpg_"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3MDPXp5-X80r"
      },
      "source": [
        "# Scraper for Twitter \n",
        "\n",
        "Package: https://github.com/Mottl/GetOldTweets3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8ZP7VA5LYQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install GetOldTweets3 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vp7x7kWeYABh",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "import GetOldTweets3 as got\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbo-UBAOKpeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "f711763d-3656-408f-97f8-b0475a84ff04"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wWyvF9CQykPz"
      },
      "source": [
        "## Scraping\n",
        "**Output**: dataframe and csv of tweets for given criterias\n",
        "___\n",
        "\n",
        "Collecting all of the tweets for the given topic, for given period of time, and maximum number of tweets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF-M3Pv-Xofx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic = 'virus'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLjWUDQKpeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_query = 'коронавірус'\n",
        "since_date = '2020-01-01'\n",
        "until_date = str(datetime.date(datetime.now()))\n",
        "count = 5000\n",
        "\n",
        "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(until_date).setMaxTweets(count)# Creation of list that contains all tweets\n",
        "tweets = got.manager.TweetManager.getTweets(tweetCriteria)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgeM_tyxKoUR",
        "colab_type": "code",
        "outputId": "dcc42684-a529-4e01-f7c1-1d527b62a038",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "len(tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9yWeBZH5IeJ",
        "colab_type": "text"
      },
      "source": [
        "Some tweets are quite short (just a picture without text, one or two words, or just one hashtag). To get more relevant information, we are going to use only long tweets (more than 40 symbols)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0n0vYXeygko",
        "colab_type": "code",
        "outputId": "12fca1fb-f95b-4bd0-c560-40864f443c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "tweets_long = [tweet.text for tweet in tweets if len(tweet.text)>40]\n",
        "len(tweets_long)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4761"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACwFfIq1J3fC",
        "colab_type": "text"
      },
      "source": [
        "Some are retweets or quotes, so are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VJ4Cb567i1Y",
        "colab_type": "code",
        "outputId": "94547e67-0b46-4985-de52-b941d08876c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "tweets_long = list(set(tweets_long))\n",
        "len(tweets_long)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4537"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJTv1HVu5j8L",
        "colab_type": "text"
      },
      "source": [
        "So from 5000 tweets we parsed, 4761 may be relevant and only 4537 are unique."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWeiif-lKpfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to save each tweet to separete .txt file (encoding utf8) (needed for making a taxonomy using ConCat Studio)\n",
        "def to_sep_files(tweets):\n",
        "  i = 1\n",
        "  for tweet in tweets:\n",
        "      text_file = open('files2/' + str(i) + '.txt', \"w\", encoding='utf8')\n",
        "      n = text_file.write(tweet)\n",
        "      text_file.close()\n",
        "      i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftd7n7F3wyNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_sep_files(tweets_long)\n",
        "!zip -r /content/file.zip /content/Folder_To_Zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mkZwOXZKpfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text of the tweets to one txt\n",
        "def to_file(tweets, name):\n",
        "  with open(f\"output/{name}.txt\", \"w\") as file:\n",
        "      for tweet in tweets:\n",
        "          file.write(re.sub(r\"\\s$\",\"\",tweet) + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45WY7CZ9yFqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_file(tweets_long, 'initial_text')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtEwn93wKpfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text of tweets to the dataframe\n",
        "df = pd.DataFrame(tweets_long, columns = ['Text'])\n",
        "df.to_csv(topic+'_initial.csv', sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC5Th5OtKpfs",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning the data\n",
        "**Output**: same but without punctuation, mentions or links\n",
        "___\n",
        "Crean the text from any kind of irrelevant symbols, including numbers, punctuation, links, mentions, russian letters, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVThGK050MKR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cleaned_text = tweets_long.copy()\n",
        "\n",
        "###OR if you just want to load already scraped tweets\n",
        "cleaned_text = [text[0] for text in pd.read_csv(f'{topic}_initial.csv', index_col=0).to_numpy()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW8YLktEKpfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(cleaned_text)):\n",
        "    cleaned_text[i] = re.sub(r'((http\\S+)|(@\\w+)|(\\s$)|[a-zA-Z0-9]|\\]|\\[|[«»!#@$%^&*()=_+-|;\\':\",.<>?…])', '', cleaned_text[i], flags=re.MULTILINE).lower()\n",
        "    cleaned_text[i] = re.sub(r'  ', ' ', cleaned_text[i])\n",
        "    #delete words that use russian symbols\n",
        "    cleaned_text[i] = re.sub(r'( \\w*(э|ы)\\w*)', ' ', cleaned_text[i]).split()\n",
        "    #delete additional bad symbols that can't be put in the RegEx\n",
        "    cleaned_text[i] = [w.replace('—', '') for w in cleaned_text[i]]\n",
        "    cleaned_text[i] = [w.replace('–', '') for w in cleaned_text[i]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViVZF1CrKpgB",
        "colab_type": "text"
      },
      "source": [
        "## Getting rid of stop words\n",
        "**Output**: same but without stop words\n",
        "___\n",
        "\n",
        "Clean the text from all words that have no meaning for our purposes. Taking into account specifics of spoken Ukrainian language (the one that is commonly used for the tweets) Russian stop words are also added.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buJWJO2xKpgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words_ru = ['вам', 'да', 'том', 'раз', 'всего', 'чтоб', 'можно', 'что','где', 'а', 'этого', 'кем', 'она', 'нему', 'тем', 'им', 'лет', 'мною', 'меня', 'сие', 'того', 'на', 'уж', 'какой', 'за', 'никогда', 'чуть', 'конечно', 'вся', 'без', 'он', 'все', 'той', 'ест', 'надо', 'как', 'но', 'одни', 'здесь', 'нельзя', 'для', 'этой', 'сейчас', 'бы', 'хоть', 'кому', 'были', 'через', 'коли', 'лучше', 'могу', 'такой', 'одна', 'ими', 'чего', 'при', 'у', 'даже', 'ведь', 'мое', 'иногда', 'тогда', 'будет', 'весь', 'наш', 'было', 'этих', 'его', 'или', 'опять', 'нею', 'нем', 'в', 'тот', 'когда', 'наконец', 'по', 'мой', 'перед', 'моя', 'чтобы', 'чей', 'до', 'одно', 'нее', 'чья', 'о', 'будто', 'два', 'вас', 'после', 'что', 'более', 'тех', 'с', 'моею', 'ему', 'ли', 'впрочем', 'ваш', 'потом', 'какая', 'другой', 'вон', 'всю', 'них', 'моем', 'три', 'ней', 'моей', 'теперь', 'нибудь', 'мол', 'об', 'разве', 'вниз', 'куда', 'лишь', 'я', 'нас', 'тут', 'много', 'ним', 'сей', 'мне', 'сам', 'есть', 'потому', 'больше', 'над', 'себе', 'всех', 'дай', 'мои', 'про', 'к', 'ей', 'вот', 'ето', 'ну', 'эта', 'тоже', 'их', 'они', 'со', 'если', 'вдруг', 'тебя', 'будь', 'оно', 'свою', 'руб', 'один', 'во', 'едим', 'себя', 'ваше', 'чем', 'то', 'может', 'еще', 'уже', 'кто', 'под', 'мы', 'ее', 'только', 'нам', 'мою', 'там', 'хорошо', 'нашу', 'же', 'между', 'всегда', 'совсем', 'ними', 'вы', 'иже', 'из', 'зачем', 'так', 'ж', 'почти', 'него', 'от', 'либо',  'и', 'ничего']\n",
        "stop_words_uk = ['який', 'яка', 'які', 'на','щоб','цей','той','чи','від','під','це','вам','шо','б', 'мій', 'y', 'той', 'тоді', 'тим', 'вже', 'чим', 'цей','тож', 'отак', 'із', 'бо', 'але','і','та','її','вона','ти','тобі','що', 'як','ще','в','або','а','от','у','з', 'які', 'чим', 'це', 'ці','й']\n",
        "stop_words = stop_words_ru + stop_words_uk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deO55td7KpgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for j in range(len(cleaned_text)):\n",
        "    for i in ((cleaned_text[j])):\n",
        "        if i in stop_words:\n",
        "            cleaned_text[j].remove(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPOal22nKpga",
        "colab_type": "code",
        "outputId": "7cf63679-d589-456b-fca9-b5f510072ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "[\" \".join(text) for text in cleaned_text[:5]]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['безпека лікарів працівників швидкої захисні костюми кількість заражень лікарів невпинно росте цифра біля тис людей ми маємо забезпечити належний захист працівникам швидких медичної галузі зоб вони могли захищати коронавірус безпекамедиківшвидкоі',\n",
              " 'знову несподіваний сюжет думала пропустять цього разу андрія холодова півтора місяця провів карантині кіпрі кажуть через те хворів коронавірус не зовсім настрій у моїй стрічці',\n",
              " 'лабораторія ухані відкидає звинувачення створенні нового коронавірусу уханський інститут вірусології керівник уханського інституту вірусології заперечує новий коронавірус могли створити його установі він наголошує поки немає достовірних',\n",
              " 'кордоном коронавірусу одужали пів сотні українців новини новости тсн коронавірус коронавирус',\n",
              " 'добу кременецькому районі виявили хворих коронавірус карта поширення']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zkvPttuKpf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame([\" \".join(text) for text in cleaned_text], columns = ['Text'])\n",
        "df.to_csv(topic + \"_cleaned.csv\", sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTFC1Db4Kpgi",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatisation\n",
        "output: dataframe and csv of tweets for given criterias lemmatized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmNwTg3PRW1",
        "colab_type": "text"
      },
      "source": [
        "As long as official pymorphy2 that can be installed from PyPi is not supporting ukrainian, and the latest version from their gitgub is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt5yiL8HD5uH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install git+https://github.com/JuliaZavalnuk/pymorphy2.git \n",
        "#https://github.com/kmike/pymorphy2\n",
        "!pip install pymorphy2-dicts-uk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgZ0jgrPKpgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymorphy2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-l1vXMRL--Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#setting library paths, so the libraries can be used\n",
        "morph = pymorphy2.MorphAnalyzer(path='/usr/local/lib/python3.6/dist-packages/pymorphy2_dicts_uk/data/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82W9o3wcdMIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatized_text = cleaned_text.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_Ok_LLKpgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for j in range(len(lemmatized_text)):\n",
        "    for i in range(len(lemmatized_text[j])):\n",
        "        lemmatized_text[j][i]= morph.parse(lemmatized_text[j][i])[0].normal_form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDFSF1m5EMAm",
        "colab_type": "code",
        "outputId": "9db49f09-1cee-4cce-9a72-197719d37cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "[\" \".join(text) for text in lemmatized_text[:5]]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['безпека лікарів працівник швидкий захисний костюм кількість зараження лікарів невпинно ріст цифра біля тис люди ми мати забезпечити належний захист працівник швидкий медичний галуза зоб вони могти захищати коронавірус безпекамедиківшвидкоа',\n",
              " 'знову несподіваний сюжет думати пропустити цей раз андрій холодовий півтора місяць провести карантин кіпр кажуть через той хворіти коронавірус не зовсім настрій у мій стрічка',\n",
              " 'лабораторія ухань відкидати звинувачення створення новий коронавірус уханський інститут вірусологія керівник уханський інститут вірусологія заперечувати новий коронавірус могти створити йога установа він наголошувати поки немає достовірний',\n",
              " 'кордон коронавірус одужати пів сотня українець новина новість тсн коронавірус коронавирус',\n",
              " 'доба кременецький район виявити хворий коронавірус карта поширення']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRLD1IMDKpg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = [\" \".join(tweet) for tweet in lemmatized_text]\n",
        "df = pd.DataFrame(temp, columns = ['Text'])\n",
        "df.to_csv(topic + \"_lemmatized.csv\", sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiwCnYCh8Vni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open(topic + \"_lemmatized_file\", 'wb') as files:\n",
        "  pickle.dump(lemmatized_text, files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s7B3rLm3NZW",
        "colab_type": "text"
      },
      "source": [
        "Save it all to the .zip for working with ConCat Studio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1JH0hnD1Umo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_sep_files([\" \".join(tweet) for tweet in lemmatized_text])\n",
        "!zip -r /content/file2.zip /content/files2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taB4qv1gKpg_",
        "colab_type": "text"
      },
      "source": [
        "## Stemming\n",
        "output: dataframe and csv of tweets for given criterias stemmed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8h_7WWCRhsD",
        "colab_type": "text"
      },
      "source": [
        "Stemming won't be used for our purpuses, but in general, was fun to try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl1DKZH-KphB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import RussianStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCJNEjbKphI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = RussianStemmer(False)\n",
        "\n",
        "stemmed_text = cleaned_text.copy()\n",
        "for j in range(len(stemmed_text)):\n",
        "    for i in range(len(stemmed_text[j])):\n",
        "        stemmed_text[j][i]= stemmer.stem(stemmed_text[j][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFVZ-TsHKphP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = [[tweet] for tweet in stemmed_text]\n",
        "df = pd.DataFrame(temp, columns = ['Text'])\n",
        "df.to_csv(topic + \"_stemmed\", sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}