{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3MDPXp5-X80r"
      },
      "source": [
        "# Scraper for Twitter \n",
        "\n",
        "Package: https://github.com/Mottl/GetOldTweets3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8ZP7VA5LYQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "3efb6bff-bc87-4072-cb64-4dd07078eb65"
      },
      "source": [
        "!pip install GetOldTweets3 "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GetOldTweets3\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/f4/a00c2a7c90801abc875325bb5416ce9090ac86d06a00cc887131bd73ba45/GetOldTweets3-0.0.11-py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Collecting pyquery>=1.2.10\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
            "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vp7x7kWeYABh",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "import GetOldTweets3 as got\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbo-UBAOKpeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "5b7319d3-6dc1-4028-a802-9e12ceb05afc"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wWyvF9CQykPz"
      },
      "source": [
        "## Scraping\n",
        "output: dataframe and csv of tweets for given criterias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQLjWUDQKpeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topic = 'virus'\n",
        "text_query = 'коронавірус'\n",
        "since_date = '2020-01-01'\n",
        "until_date = str(datetime.date(datetime.now()))\n",
        "count = 700\n",
        "\n",
        "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(until_date).setMaxTweets(count)# Creation of list that contains all tweets\n",
        "tweets = got.manager.TweetManager.getTweets(tweetCriteria)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjSLlZKZKpe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text of tweets to list\n",
        "text_tweets = [tweet.text for tweet in tweets]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWeiif-lKpfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 1\n",
        "for tweet in tweets:\n",
        "    text_file = open('output/' + str(i) + '.txt', \"w\", encoding='utf8')\n",
        "    n = text_file.write(tweet.text)\n",
        "    text_file.close()\n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mkZwOXZKpfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initial text of the tweets to txt\n",
        "with open(\"output/virus_initial.txt\", \"w\") as file:\n",
        "    for t in range(len(tweets)):\n",
        "        file.write(str(t) +'\\t'+ tweets[t].text + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtEwn93wKpfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#text of tweets to the dataframe\n",
        "df = pd.DataFrame(text_tweets, columns = ['Text'])\n",
        "df.to_csv(topic, sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoGZ-e5yKpfh",
        "colab_type": "code",
        "colab": {},
        "outputId": "52c761be-b5ec-4f8f-9b56-3aa70652a76c"
      },
      "source": [
        "for i in df:\n",
        "    i.to_csv(str(i)+ '.txt',index=False, sep='\\t', encoding = 'utf-8')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'to_csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e44b83a1a493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC5Th5OtKpfs",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning the data\n",
        "output: same but without punctuation, mentions or links"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW8YLktEKpfu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_text = text_tweets.copy()\n",
        "\n",
        "for i in range(len(cleaned_text)):\n",
        "    cleaned_text[i] = re.sub(r'((http\\S+)|(@\\w+)|(#\\w+)|[a-zA-Z0-9]|\\]|\\[|[«»!#@$%^&*()=_+-|;\\':\",.<>?])', '', cleaned_text[i], flags=re.MULTILINE).lower()\n",
        "    cleaned_text[i] = re.sub(r'  ', ' ', cleaned_text[i]).split()\n",
        "    # Additional bad symbols that can't be put in the RegEx\n",
        "    cleaned_text[i] = [w.replace('—', '') for w in cleaned_text[i]]\n",
        "    cleaned_text[i] = [w.replace('–', '') for w in cleaned_text[i]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zkvPttuKpf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame([[tweet] for tweet in cleaned_text], columns = ['Text'])\n",
        "df.to_csv(topic + \"_cleaned\", sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViVZF1CrKpgB",
        "colab_type": "text"
      },
      "source": [
        "## Getting rid of stop words\n",
        "output: same but without stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buJWJO2xKpgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words_ru = ['вам', 'да', 'том', 'раз', 'всего', 'чтоб', 'можно', 'что','где', 'а', 'этого', 'кем', 'она', 'нему', 'тем', 'им', 'лет', 'мною', 'меня', 'сие', 'был', 'была', 'того', 'на', 'уж', 'какой', 'за', 'никогда', 'чуть', 'конечно', 'вся', 'без', 'он', 'все', 'той', 'ест', 'надо', 'как', 'но', 'одни', 'здесь', 'нельзя', 'для', 'этой', 'сейчас', 'бы', 'хоть', 'кому', 'были', 'через', 'коли', 'лучше', 'могу', 'такой', 'одна', 'ими', 'чего', 'при', 'у', 'даже', 'ведь', 'мое', 'иногда', 'тогда', 'будет', 'весь', 'наш', 'было', 'этих', 'его', 'или', 'опять', 'нею', 'нем', 'в', 'тот', 'когда', 'наконец', 'по', 'мой', 'перед', 'моя', 'чтобы', 'чей', 'до', 'одно', 'нее', 'чья', 'о', 'будто', 'два', 'вас', 'после', 'что', 'более', 'тех', 'с', 'моею', 'ему', 'ли', 'впрочем', 'ваш', 'потом', 'какая', 'другой', 'вон', 'всю', 'них', 'моем', 'три', 'это', 'ней', 'моей', 'теперь', 'нибудь', 'эту', 'мол', 'об', 'разве', 'вниз', 'куда', 'лишь', 'я', 'нас', 'тут', 'много', 'ним', 'сей', 'мне', 'сам', 'есть', 'потому', 'больше', 'над', 'себе', 'быть', 'всех', 'дай', 'мои', 'про', 'к', 'ей', 'вот', 'ето', 'ну', 'эта', 'тоже', 'их', 'они', 'со', 'если', 'вдруг', 'тебя', 'будь', 'оно', 'свою', 'руб', 'один', 'во', 'едим', 'себя', 'ваше', 'этот', 'чем', 'то', 'может', 'еще', 'уже', 'кто', 'под', 'мы', 'ее', 'только', 'нам', 'мою', 'там', 'хорошо', 'нашу', 'же', 'между', 'тыс', 'всегда', 'совсем', 'ними', 'вы', 'иже', 'из', 'зачем', 'ты', 'так', 'ж', 'почти', 'него', 'от', 'либо', 'эти', 'и', 'ничего', 'этом']\n",
        "stop_words_uk = ['шо','б','тоді', 'тим','чим', 'тож', 'отак', 'із', 'бо', 'але','і','та','її','вона','ти','тобі','що', 'як','ще','в','або','а','от','у','з', 'які', 'чим', 'це', 'ці','й']\n",
        "stop_words = stop_words_ru + stop_words_uk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deO55td7KpgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for j in range(len(cleaned_text)):\n",
        "    for i in ((cleaned_text[j])):\n",
        "        if i in stop_words:\n",
        "            cleaned_text[j].remove(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPOal22nKpga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(cleaned_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTFC1Db4Kpgi",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatisation\n",
        "output: dataframe and csv of tweets for given criterias lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgZ0jgrPKpgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymorphy2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD_Ok_LLKpgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "lemmatized_text = cleaned_text.copy()\n",
        "for j in range(len(lemmatized_text)):\n",
        "    for i in range(len(lemmatized_text[j])):\n",
        "        lemmatized_text[j][i]= morph.parse(lemmatized_text[j][i])[0].normal_form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRLD1IMDKpg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = [[tweet] for tweet in lemmatized_text]\n",
        "df = pd.DataFrame(temp, columns = ['Text'])\n",
        "df.to_csv(topic + \"_lemmatized\", sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taB4qv1gKpg_",
        "colab_type": "text"
      },
      "source": [
        "## Stemming\n",
        "output: dataframe and csv of tweets for given criterias stemmed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hl1DKZH-KphB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import RussianStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCJNEjbKphI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = RussianStemmer(False)\n",
        "\n",
        "stemmed_text = cleaned_text.copy()\n",
        "for j in range(len(stemmed_text)):\n",
        "    for i in range(len(stemmed_text[j])):\n",
        "        stemmed_text[j][i]= stemmer.stem(stemmed_text[j][i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFVZ-TsHKphP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = [[tweet] for tweet in stemmed_text]\n",
        "df = pd.DataFrame(temp, columns = ['Text'])\n",
        "df.to_csv(topic + \"_stemmed\", sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPQivTPBKphV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLkXrjeIKphf",
        "colab_type": "text"
      },
      "source": [
        "## additional stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54rhT5wfZVXD",
        "colab": {}
      },
      "source": [
        "def username_tweets_to_csv(username, count):\n",
        "    tweetCriteria = got.manager.TweetCriteria().setUsername(username).setMaxTweets(count)\n",
        "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "    user_tweets = [[tweet.date, tweet.text, tweet.geo] for tweet in tweets]\n",
        "    tweets_df = pd.DataFrame(user_tweets, columns = ['Datetime', 'Text' , 'Geo'])\n",
        "    tweets_df.to_csv('{}-{}k-tweets.csv'.format(username, int(count/1000)), sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JSjpix_9A5e6",
        "colab": {}
      },
      "source": [
        "def text_query_to_csv(text_query, count, name):\n",
        "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setMaxTweets(count)\n",
        "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "    text_tweets = [[tweet.date, tweet.text] for tweet in tweets]\n",
        "    tweets_df = pd.DataFrame(text_tweets, columns = ['Datetime', 'Text'])\n",
        "    #tweets_df.to_csv('{}-{}k-tweets.csv'.format(text_query, int(count/1000)), sep=',')\n",
        "    tweets_df.to_csv(name, sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kQfm6LdhZafM",
        "colab": {}
      },
      "source": [
        "# Input username(s) to scrape tweets and name csv file\n",
        "username = 'tired_waffle'\n",
        "count = 5000\n",
        "username_tweets_to_csv(username, count)\n",
        "\n",
        "#text_query = 'погода Київ'\n",
        "#count = 100\n",
        "#text_query_to_csv(text_query, count, \"try_query\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}