{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MDPXp5-X80r"
   },
   "source": [
    "# Scraper for Twitter \n",
    "\n",
    "Package: https://github.com/Mottl/GetOldTweets3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "x8ZP7VA5LYQx",
    "outputId": "3efb6bff-bc87-4072-cb64-4dd07078eb65"
   },
   "outputs": [],
   "source": [
    "!pip install GetOldTweets3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "vp7x7kWeYABh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Sbo-UBAOKpeU",
    "outputId": "5b7319d3-6dc1-4028-a802-9e12ceb05afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWyvF9CQykPz"
   },
   "source": [
    "## Scraping\n",
    "output: dataframe and csv of tweets for given criterias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jQLjWUDQKpeo"
   },
   "outputs": [],
   "source": [
    "topic = 'virus'\n",
    "text_query = 'коронавірус'\n",
    "since_date = '2020-01-01'\n",
    "until_date = str(datetime.date(datetime.now()))\n",
    "count = 700\n",
    "\n",
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setSince(since_date).setUntil(until_date).setMaxTweets(count)# Creation of list that contains all tweets\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zjSLlZKZKpe1"
   },
   "outputs": [],
   "source": [
    "#text of tweets to list\n",
    "text_tweets = [tweet.text for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fWeiif-lKpfB"
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "for tweet in tweets:\n",
    "    text_file = open('output/' + str(i) + '.txt', \"w\", encoding='utf8')\n",
    "    n = text_file.write(tweet.text)\n",
    "    text_file.close()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2mkZwOXZKpfN"
   },
   "outputs": [],
   "source": [
    "#initial text of the tweets to txt\n",
    "with open(\"output/virus_initial.txt\", \"w\") as file:\n",
    "    for t in range(len(tweets)):\n",
    "        file.write(str(t) +'\\t'+ tweets[t].text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gtEwn93wKpfZ"
   },
   "outputs": [],
   "source": [
    "#text of tweets to the dataframe\n",
    "df = pd.DataFrame(text_tweets, columns = ['Text'])\n",
    "df.to_csv(topic, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "LoGZ-e5yKpfh",
    "outputId": "52c761be-b5ec-4f8f-9b56-3aa70652a76c"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e44b83a1a493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "for i in df:\n",
    "    i.to_csv(str(i)+ '.txt',index=False, sep='\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC5Th5OtKpfs"
   },
   "source": [
    "## Cleaning the data\n",
    "output: same but without punctuation, mentions or links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jW8YLktEKpfu"
   },
   "outputs": [],
   "source": [
    "cleaned_text = text_tweets.copy()\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    cleaned_text[i] = re.sub(r'((http\\S+)|(@\\w+)|(#\\w+)|[a-zA-Z0-9]|\\]|\\[|[«»!#@$%^&*()=_+-|;\\':\",.<>?])', '', cleaned_text[i], flags=re.MULTILINE).lower()\n",
    "    cleaned_text[i] = re.sub(r'  ', ' ', cleaned_text[i]).split()\n",
    "    # Additional bad symbols that can't be put in the RegEx\n",
    "    cleaned_text[i] = [w.replace('—', '') for w in cleaned_text[i]]\n",
    "    cleaned_text[i] = [w.replace('–', '') for w in cleaned_text[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1zkvPttuKpf3"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[tweet] for tweet in cleaned_text], columns = ['Text'])\n",
    "df.to_csv(topic + \"_cleaned\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViVZF1CrKpgB"
   },
   "source": [
    "## Getting rid of stop words\n",
    "output: same but without stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "buJWJO2xKpgD"
   },
   "outputs": [],
   "source": [
    "stop_words_ru = ['вам', 'да', 'том', 'раз', 'всего', 'чтоб', 'можно', 'что','где', 'а', 'этого', 'кем', 'она', 'нему', 'тем', 'им', 'лет', 'мною', 'меня', 'сие', 'был', 'была', 'того', 'на', 'уж', 'какой', 'за', 'никогда', 'чуть', 'конечно', 'вся', 'без', 'он', 'все', 'той', 'ест', 'надо', 'как', 'но', 'одни', 'здесь', 'нельзя', 'для', 'этой', 'сейчас', 'бы', 'хоть', 'кому', 'были', 'через', 'коли', 'лучше', 'могу', 'такой', 'одна', 'ими', 'чего', 'при', 'у', 'даже', 'ведь', 'мое', 'иногда', 'тогда', 'будет', 'весь', 'наш', 'было', 'этих', 'его', 'или', 'опять', 'нею', 'нем', 'в', 'тот', 'когда', 'наконец', 'по', 'мой', 'перед', 'моя', 'чтобы', 'чей', 'до', 'одно', 'нее', 'чья', 'о', 'будто', 'два', 'вас', 'после', 'что', 'более', 'тех', 'с', 'моею', 'ему', 'ли', 'впрочем', 'ваш', 'потом', 'какая', 'другой', 'вон', 'всю', 'них', 'моем', 'три', 'это', 'ней', 'моей', 'теперь', 'нибудь', 'эту', 'мол', 'об', 'разве', 'вниз', 'куда', 'лишь', 'я', 'нас', 'тут', 'много', 'ним', 'сей', 'мне', 'сам', 'есть', 'потому', 'больше', 'над', 'себе', 'быть', 'всех', 'дай', 'мои', 'про', 'к', 'ей', 'вот', 'ето', 'ну', 'эта', 'тоже', 'их', 'они', 'со', 'если', 'вдруг', 'тебя', 'будь', 'оно', 'свою', 'руб', 'один', 'во', 'едим', 'себя', 'ваше', 'этот', 'чем', 'то', 'может', 'еще', 'уже', 'кто', 'под', 'мы', 'ее', 'только', 'нам', 'мою', 'там', 'хорошо', 'нашу', 'же', 'между', 'тыс', 'всегда', 'совсем', 'ними', 'вы', 'иже', 'из', 'зачем', 'ты', 'так', 'ж', 'почти', 'него', 'от', 'либо', 'эти', 'и', 'ничего', 'этом']\n",
    "stop_words_uk = ['шо','б','тоді', 'тим','чим', 'тож', 'отак', 'із', 'бо', 'але','і','та','її','вона','ти','тобі','що', 'як','ще','в','або','а','от','у','з', 'які', 'чим', 'це', 'ці','й']\n",
    "stop_words = stop_words_ru + stop_words_uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "deO55td7KpgN"
   },
   "outputs": [],
   "source": [
    "for j in range(len(cleaned_text)):\n",
    "    for i in ((cleaned_text[j])):\n",
    "        if i in stop_words:\n",
    "            cleaned_text[j].remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "MPOal22nKpga"
   },
   "outputs": [],
   "source": [
    "#print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTFC1Db4Kpgi"
   },
   "source": [
    "## Lemmatisation\n",
    "output: dataframe and csv of tweets for given criterias lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VgZ0jgrPKpgk"
   },
   "outputs": [],
   "source": [
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "pD_Ok_LLKpgv"
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "lemmatized_text = cleaned_text.copy()\n",
    "for j in range(len(lemmatized_text)):\n",
    "    for i in range(len(lemmatized_text[j])):\n",
    "        lemmatized_text[j][i]= morph.parse(lemmatized_text[j][i])[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "IRLD1IMDKpg3"
   },
   "outputs": [],
   "source": [
    "temp = [[tweet] for tweet in lemmatized_text]\n",
    "df = pd.DataFrame(temp, columns = ['Text'])\n",
    "df.to_csv(topic + \"_lemmatized\", sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taB4qv1gKpg_"
   },
   "source": [
    "## Stemming\n",
    "output: dataframe and csv of tweets for given criterias stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Hl1DKZH-KphB"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import RussianStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iaCJNEjbKphI"
   },
   "outputs": [],
   "source": [
    "stemmer = RussianStemmer(False)\n",
    "\n",
    "stemmed_text = cleaned_text.copy()\n",
    "for j in range(len(stemmed_text)):\n",
    "    for i in range(len(stemmed_text[j])):\n",
    "        stemmed_text[j][i]= stemmer.stem(stemmed_text[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NFVZ-TsHKphP"
   },
   "outputs": [],
   "source": [
    "temp = [[tweet] for tweet in stemmed_text]\n",
    "df = pd.DataFrame(temp, columns = ['Text'])\n",
    "df.to_csv(topic + \"_stemmed\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tPQivTPBKphV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLkXrjeIKphf"
   },
   "source": [
    "## additional stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "54rhT5wfZVXD"
   },
   "outputs": [],
   "source": [
    "def username_tweets_to_csv(username, count):\n",
    "    tweetCriteria = got.manager.TweetCriteria().setUsername(username).setMaxTweets(count)\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    user_tweets = [[tweet.date, tweet.text, tweet.geo] for tweet in tweets]\n",
    "    tweets_df = pd.DataFrame(user_tweets, columns = ['Datetime', 'Text' , 'Geo'])\n",
    "    tweets_df.to_csv('{}-{}k-tweets.csv'.format(username, int(count/1000)), sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JSjpix_9A5e6"
   },
   "outputs": [],
   "source": [
    "def text_query_to_csv(text_query, count, name):\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setMaxTweets(count)\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    text_tweets = [[tweet.date, tweet.text] for tweet in tweets]\n",
    "    tweets_df = pd.DataFrame(text_tweets, columns = ['Datetime', 'Text'])\n",
    "    #tweets_df.to_csv('{}-{}k-tweets.csv'.format(text_query, int(count/1000)), sep=',')\n",
    "    tweets_df.to_csv(name, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kQfm6LdhZafM"
   },
   "outputs": [],
   "source": [
    "# Input username(s) to scrape tweets and name csv file\n",
    "username = 'tired_waffle'\n",
    "count = 5000\n",
    "username_tweets_to_csv(username, count)\n",
    "\n",
    "#text_query = 'погода Київ'\n",
    "#count = 100\n",
    "#text_query_to_csv(text_query, count, \"try_query\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
